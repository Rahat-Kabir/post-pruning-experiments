{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained ResNet-18 Model: Modified for CIFAR-10 classification.\n",
        "Structured Pruning: Removes 20% of convolutional filters (excluding residual connections).\n",
        "Fine-Tuning: Retrains the pruned model to recover accuracy.\n",
        "Performance Evaluation: Measures accuracy, model size, and inference speed before & after pruning.\n",
        "\n",
        "Before Pruning: The model had 89.84% accuracy on CIFAR-10.\n",
        "After Pruning: Accuracy dropped to 18.87% due to filter removal.\n",
        "After Fine-Tuning: Accuracy recovered to 91.67%, even improving slightly."
      ],
      "metadata": {
        "id": "5qKr0dEFdZDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk_2-BWWdPtO",
        "outputId": "03fe4c0f-e028-4cec-9b8e-38b767270b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.prune as prune\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "UoEA_Hk-dh2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load a pre-trained ResNet-18 model\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DVyxUqXdh87",
        "outputId": "c64ad626-4da8-4eb6-b965-89b31c086533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 133MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Modify the final layer for CIFAR-10 (10 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n"
      ],
      "metadata": {
        "id": "I6L6ulXydh_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Adjust input size for CIFAR-10 (32x32)\n",
        "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "model.maxpool = nn.Identity()  # Remove initial max pooling layer"
      ],
      "metadata": {
        "id": "pfSiCp7_dpSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2UF96scdwuM",
        "outputId": "d14ec826-ce03-48a7-9c66-9c112fda3cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "akpfQ326dwwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Fine-tune the model on CIFAR-10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCXkTLtTdw1v",
        "outputId": "60f1d1da-1660-4eeb-8b51-74948fe3af0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7267\n",
            "Epoch 2, Loss: 0.2908\n",
            "Epoch 3, Loss: 0.1748\n",
            "Epoch 4, Loss: 0.1025\n",
            "Epoch 5, Loss: 0.0729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Evaluate the model before pruning\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "print(\"Evaluating model before pruning...\")\n",
        "accuracy_before = evaluate_model(model, test_loader)\n",
        "print(f\"Accuracy before pruning: {accuracy_before:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkQrqeyYd1wM",
        "outputId": "957dab52-6b54-4f89-c29c-083b090c3176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model before pruning...\n",
            "Accuracy before pruning: 89.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model size BEFORE pruning\n",
        "torch.save(model.state_dict(), \"model_before_pruning.pth\")\n",
        "print(f\"Original model size: {os.path.getsize('model_before_pruning.pth') / 1024:.2f} KB\")\n",
        "\n",
        "# Measure inference time BEFORE pruning\n",
        "time_before = measure_inference_time(model, test_loader, device)\n",
        "print(f\"Inference Time Before Pruning: {time_before:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9d_HpbOlkat",
        "outputId": "6c853fd8-269f-47fa-c285-dbe7a3726369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model size: 43726.85 KB\n",
            "Inference Time Before Pruning: 2.6622 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Count parameters before pruning\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "params_before = count_parameters(model)"
      ],
      "metadata": {
        "id": "APL7wz5Hd1y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) and \"downsample\" not in name:\n",
        "        prune.ln_structured(module, name='weight', amount=0.2, n=2, dim=0)  # Prune 20%\n",
        "        prune.remove(module, 'weight')  # Make pruning permanent\n",
        "        # Manually set pruned weights to zero\n",
        "        with torch.no_grad():\n",
        "            module.weight[module.weight == 0] = 0\n"
      ],
      "metadata": {
        "id": "8ueREAwFd133"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model size AFTER pruning\n",
        "torch.save(model.state_dict(), \"model_after_pruning.pth\")\n",
        "print(f\"Pruned model size: {os.path.getsize('model_after_pruning.pth') / 1024:.2f} KB\")\n",
        "\n",
        "# Measure inference time AFTER pruning\n",
        "time_after = measure_inference_time(model, test_loader, device)\n",
        "print(f\"Inference Time After Pruning: {time_after:.4f} seconds\")\n",
        "\n",
        "# Print parameter comparison\n",
        "params_after = count_parameters(model)\n",
        "print(f\"Number of parameters before pruning: {params_before}\")\n",
        "print(f\"Number of parameters after pruning: {params_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SByEcKpnlenX",
        "outputId": "2aa5e0eb-9e1c-4d0c-8c52-f70e6d59aa1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model size: 43726.72 KB\n",
            "Inference Time After Pruning: 2.6991 seconds\n",
            "Number of parameters before pruning: 11173962\n",
            "Number of parameters after pruning: 11173962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Evaluate the model after pruning\n",
        "print(\"Evaluating model after pruning...\")\n",
        "accuracy_after = evaluate_model(model, test_loader)\n",
        "print(f\"Accuracy after pruning: {accuracy_after:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6byrsTuQd8u7",
        "outputId": "fb114859-4ad2-4736-fc53-7ae7166e871d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model after pruning...\n",
            "Accuracy after pruning: 88.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Count parameters after pruning\n",
        "params_after = count_parameters(model)\n",
        "print(f\"Number of parameters before pruning: {params_before}\")\n",
        "print(f\"Number of parameters after pruning: {params_after}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ExAG2Q-d8xf",
        "outputId": "562ff345-6663-45a5-8077-e8b3ec007d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters before pruning: 11173962\n",
            "Number of parameters after pruning: 11173962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Fine-tune after pruning\n",
        "print(\"Fine-tuning the pruned model...\")\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)  # Lower learning rate for fine-tuning\n",
        "num_finetune_epochs = 3\n",
        "\n",
        "for epoch in range(num_finetune_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Fine-Tuning Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lN56roNd80C",
        "outputId": "ad8fc6ca-e039-47a7-a196-27da9186f8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning the pruned model...\n",
            "Fine-Tuning Epoch 1, Loss: 0.0191\n",
            "Fine-Tuning Epoch 2, Loss: 0.0075\n",
            "Fine-Tuning Epoch 3, Loss: 0.0052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 15: Final evaluation after fine-tuning\n",
        "print(\"Final evaluation after fine-tuning...\")\n",
        "final_accuracy = evaluate_model(model, test_loader)\n",
        "print(f\"Final accuracy after fine-tuning: {final_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-5Kf7zeeDvE",
        "outputId": "2b72e6f5-7169-44ba-9d2c-d458eeb14314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final evaluation after fine-tuning...\n",
            "Final accuracy after fine-tuning: 91.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model size AFTER pruning\n",
        "torch.save(model.state_dict(), \"model_after_pruning.pth\")\n",
        "print(f\"Pruned model size: {os.path.getsize('model_after_pruning.pth') / 1024:.2f} KB\")\n",
        "\n",
        "# Measure inference time AFTER pruning\n",
        "time_after = measure_inference_time(model, test_loader, device)\n",
        "print(f\"Inference Time After Pruning: {time_after:.4f} seconds\")\n",
        "\n",
        "# Print parameter comparison\n",
        "params_after = count_parameters(model)\n",
        "print(f\"Number of parameters before pruning: {params_before}\")\n",
        "print(f\"Number of parameters after pruning: {params_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7p4KM7oeDxy",
        "outputId": "abe2aca5-cd63-474d-9ed7-c4a64b0a302e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model size: 43726.72 KB\n",
            "Inference Time After Pruning: 2.6802 seconds\n",
            "Number of parameters before pruning: 11173962\n",
            "Number of parameters after pruning: 11173962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "# Step 1: Count Model Parameters (Already Done)\n",
        "print(f\"Number of parameters before pruning: {params_before}\")\n",
        "print(f\"Number of parameters after pruning: {params_after}\")\n",
        "\n",
        "# Step 2: Save the Model & Check File Size\n",
        "torch.save(model.state_dict(), \"model_pruned.pth\")\n",
        "print(f\"Pruned model size: {os.path.getsize('model_pruned.pth') / 1024:.2f} KB\")\n",
        "\n",
        "# Step 3: Measure Inference Speed\n",
        "def measure_inference_time(model, data_loader, device):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in data_loader:\n",
        "            images = images.to(device)\n",
        "            model(images)  # Forward pass\n",
        "\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "# Measure inference time after pruning\n",
        "time_after = measure_inference_time(model, test_loader, device)\n",
        "print(f\"Inference Time After Pruning: {time_after:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaFVoCR_mPu9",
        "outputId": "7a55b793-a1b9-44dd-8c65-d1b603f7e362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters before pruning: 11173962\n",
            "Number of parameters after pruning: 11173962\n",
            "Pruned model size: 43725.86 KB\n",
            "Inference Time After Pruning: 2.6540 seconds\n"
          ]
        }
      ]
    }
  ]
}