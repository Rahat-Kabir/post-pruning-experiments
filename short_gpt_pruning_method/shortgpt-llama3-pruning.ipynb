{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üöÄ Fine-Tuning & Evaluating LLaMA 3 with ShortGPT\n\nIn this notebook, we will use the **ShortGPT** code (from the research paper) to fine-tune and evaluate a **LLaMA 3** model.\n\nOur goals:\n- üß© Fine-tune LLaMA 3 using ShortGPT methods.\n- üß™ Evaluate the model on benchmark tasks.\n- ‚öôÔ∏è Learn to use tools like Hugging Face and lm-eval.\n\nThis notebook is beginner-friendly! We‚Äôll explain each step simply and clearly.\n\nLet‚Äôs get started! üöÄ\n","metadata":{}},{"cell_type":"code","source":"!pip install datasets lm_eval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# importing the required libraries\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, Trainer, TrainingArguments\nfrom collections import OrderedDict\nfrom typing import List, Optional\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nimport torch\nfrom torch.utils.data import DataLoader\nfrom lm_eval import evaluator, tasks","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:55:48.052355Z","iopub.status.busy":"2025-03-15T11:55:48.052047Z","iopub.status.idle":"2025-03-15T11:56:11.087829Z","shell.execute_reply":"2025-03-15T11:56:11.087111Z","shell.execute_reply.started":"2025-03-15T11:55:48.052332Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:11.088701Z","iopub.status.busy":"2025-03-15T11:56:11.088489Z","iopub.status.idle":"2025-03-15T11:56:11.115810Z","shell.execute_reply":"2025-03-15T11:56:11.114927Z","shell.execute_reply.started":"2025-03-15T11:56:11.088683Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3459342ff0154438940ce98708468320","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token has not been saved to git credential helper.\n"]}],"execution_count":3},{"cell_type":"code","source":"def layer_removal(\n    model: nn.Module,\n    layers_to_remove: OrderedDict\n):\n    \"\"\"\n    Generic removal implementation\n    \"\"\"\n\n    for layer_name, layer_idx in layers_to_remove.items():\n        modules = layer_name.split(\".\")\n        mod = model\n        for m in modules[:-1]:\n            mod = getattr(mod, m)\n        \n        if layer_idx is None:\n            delattr(mod, modules[-1])\n        else:\n            delattr(mod, modules[-1])[layer_idx]","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:17.163635Z","iopub.status.busy":"2025-03-15T11:56:17.163311Z","iopub.status.idle":"2025-03-15T11:56:17.168457Z","shell.execute_reply":"2025-03-15T11:56:17.167616Z","shell.execute_reply.started":"2025-03-15T11:56:17.163610Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def block_influence(\n    input_hidden_state: torch.Tensor,\n    output_hidden_state: torch.Tensor,\n    angular=False,\n):\n    \"\"\"\n    input_hidden_state: B, S, D\n    output_hidden_state: B, S, D\n    \"\"\"\n    _, _, d = input_hidden_state.shape\n    input_hidden_state = input_hidden_state.reshape(-1, d)\n    output_hidden_state = output_hidden_state.reshape(-1, d)\n\n    norm_input = input_hidden_state.norm(dim=-1, keepdim=True)\n    norm_output = output_hidden_state.norm(dim=-1, keepdim=True)\n\n    sim = (input_hidden_state @ output_hidden_state.T) / (norm_input * norm_output)\n    sim = sim.diagonal().nan_to_num(nan=0.5)\n\n    if angular:\n        return (torch.arccos(sim) / torch.pi)\n\n    return 1 - sim","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:18.125812Z","iopub.status.busy":"2025-03-15T11:56:18.125501Z","iopub.status.idle":"2025-03-15T11:56:18.130773Z","shell.execute_reply":"2025-03-15T11:56:18.129891Z","shell.execute_reply.started":"2025-03-15T11:56:18.125787Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ShortHFModel():\n\n    def __init__(self, model_name: str, layers_path: str, n_prune_layers: Optional[int] = None):\n        \"\"\"\n        HuggingFace Model Wrapper\n\n        Args:\n            model_name (str): HuggingFace model name\n            layers_path (str): String in dot notation demonstrating how to access layers of the model. Ex: \"model.layers\"\n            (Optional) n_prune_layers (int): Number of layers to prune. Defaults to None.\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n        # self.model.params = self.model.to_fp16(self.model.params)\n        self.model.to(\"cuda\")\n\n        modules = layers_path.split(\".\")\n        mod = self.model\n        for m in modules:\n            mod = getattr(mod, m)\n        self.layers = mod\n\n        self.n_prune_layers = n_prune_layers\n        self.importances = [0 for _ in self.layers]  # layer-wise importance scores\n\n    def remove_layers(\n        self,\n        layers_to_remove: Optional[List[int]] = [],\n        angular: Optional[bool] = False\n    ):\n        if angular:\n            assert self.importances, \"Need to compute importances with eval_importance()\"\n            assert self.n_prune_layers, \"Need number of layers to prune, set `n_prune_layers`\"\n            start_layer = np.argsort(np.array(self.importances[:-self.n_prune_layers+1]))[0]\n            layers_to_remove = list(range(start_layer, start_layer + self.n_prune_layers))\n        elif not layers_to_remove and self.n_prune_layers:\n            assert self.importances, \"Need to compute importances with eval_importance()\"\n            layers_to_remove = np.argsort(np.array(self.importances))[:self.n_prune_layers].tolist()\n\n        # remove layers in reverse to avoid indexing errors\n        for layer_idx in sorted(layers_to_remove, reverse=True):\n            try:\n                del self.layers[layer_idx]\n            except IndexError:\n                print(f\"layer {layer_idx} does not exist, function may have already been called\")\n                return []\n        \n        return layers_to_remove\n    \n    def compute_bi(self, hiddens: List[torch.Tensor], angular: bool):\n        n = 1\n        if angular:\n            assert self.n_prune_layers is not None, \"Set number of layers to prune to use angular importance\"\n            n = self.n_prune_layers\n\n        for i in range(len(hiddens) - n):\n            in_hidden = hiddens[i]\n            out_hidden = hiddens[i+n]\n            if angular:\n                # use only last token for angular distance as described in section 3.2\n                # https://arxiv.org/pdf/2403.17887.pdf\n                in_hidden = in_hidden[:,-1:]\n                out_hidden = out_hidden[:,-1:]\n            \n            self.importances[i] += block_influence(\n                in_hidden,\n                out_hidden,\n                angular=angular\n            ).sum().cpu().item()\n\n    @torch.inference_mode()\n    def eval_importance(\n        self,\n        prompts: List[str],\n        max_seq_len: int,\n        stride: int = 256,\n        max_gen_len: int = 0,\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        angular: Optional[bool] = False\n    ):\n        \"\"\"\n        Computes layer-wise importances over input texts.\n\n        NOTE: ShortGPT paper performs no generation during importance computation, which suggests a `max_gen_len`= 0.\n\n        Args:\n            prompts (List[str]): List of prompts.\n            max_seq_len (int): Maximum sequence length for model input, the sliding window size.\n            (Optional) stride (int): Number of tokens to skip/shift between each window inference.\n            (Optional) max_gen_len (int): Maximum length of the generated text sequence.\n            (Optional) temperature (float): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            (Optional) top_p (float): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            (Optional) angular (bool): Whether to ues angular distance. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        prompt_tokens = self.tokenizer(\n            prompts,\n            padding=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        input_ids = prompt_tokens.input_ids\n        attn_mask = prompt_tokens.attention_mask\n\n        max_prompt_len = max(len(t) for t in input_ids)\n\n        # authors use a sliding window of size 1024 with a shift of 256\n        for start in range(0, max_prompt_len, stride):\n            seq_ids = (attn_mask.sum(dim=-1) > start).nonzero().squeeze()\n            seq_ids = seq_ids.unsqueeze(0) if seq_ids.dim() == 0 else seq_ids  # ensure 2d\n            inputs = input_ids[seq_ids, start:start+max_seq_len]\n            attn = attn_mask[seq_ids, start:start+max_seq_len]\n\n            if max_gen_len == 0:\n                outputs = self.model(\n                    input_ids=inputs.to(\"cuda\"),\n                    attention_mask=attn.to(\"cuda\"),\n                    output_hidden_states=True,\n                )\n            else:\n                outputs = self.model.generate(\n                    input_ids=inputs.to(\"cuda\"),\n                    attention_mask=attn.to(\"cuda\"),\n                    max_new_tokens=max_gen_len, \n                    do_sample=True,\n                    temperature=temperature,\n                    top_p=top_p,\n                    output_hidden_states=True,\n                    return_dict_in_generate=True,\n                )\n            \n            self.compute_bi(outputs.hidden_states, angular=angular)\n\n        return","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:20.250357Z","iopub.status.busy":"2025-03-15T11:56:20.249767Z","iopub.status.idle":"2025-03-15T11:56:20.275092Z","shell.execute_reply":"2025-03-15T11:56:20.274217Z","shell.execute_reply.started":"2025-03-15T11:56:20.250309Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### üìö Loading the Dataset\n\nNow, let's load our dataset!\n\nWe are using the **\"pg19\"** dataset, which contains long English texts from books published before 1919.  \nFor this notebook, we are loading the **validation split**, and we only take a small sample of **50 examples** to keep things simple and fast.\n\nWe also use **streaming** to load the data more efficiently, especially when working with large datasets.\n\nFinally, we create a `DataLoader` to prepare the data for training or evaluation, with a `batch_size` of 1.\n\n> üìù Note: The shuffle option is commented out for now. If you want to randomize the data order, you can enable it!\n","metadata":{}},{"cell_type":"code","source":"data = load_dataset(\"pg19\",split=\"validation\",streaming=True).take(50) # loading the dataset.\ndataloader = DataLoader(\n    data,\n    batch_size=1,\n    #shuffle=True,\n)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:23.778673Z","iopub.status.busy":"2025-03-15T11:56:23.778340Z","iopub.status.idle":"2025-03-15T11:56:29.863135Z","shell.execute_reply":"2025-03-15T11:56:29.862458Z","shell.execute_reply.started":"2025-03-15T11:56:23.778644Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42dff400762c48ab8a00da1636e5ad2b","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/8.11k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"511ce8896c8a4a33bd1cf3f955da0b04","version_major":2,"version_minor":0},"text/plain":["pg19.py:   0%|          | 0.00/6.56k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":7},{"cell_type":"markdown","source":"### ü¶ô Initializing the LLaMA 3 Model (ShortGPT Version)\n\nNext, we set up our model!  \nWe are using **ShortHFModel**, which is part of the ShortGPT approach from the paper.\n\nHere's what we are doing:\n- We set `MAX_SEQ_LEN` to **1024**, which means our model will process sequences of up to 1024 tokens at a time.\n- We load the **LLaMA 3.2B model** from Hugging Face (`meta-llama/Llama-3-2-3B`).\n- We tell the model where to find its layers using `layers_path=\"model.layers\"`.\n- We set `n_prune_layers=5` ‚Äî this means we will **prune (remove) 5 layers** from the model to make it smaller and faster.\n\n> üìù Note: Pruning helps reduce the size and computation cost of large models, which is especially useful when fine-tuning!\n\nThis prepares our model for the next steps: training and evaluation! üöÄ\n","metadata":{}},{"cell_type":"code","source":"MAX_SEQ_LEN = 1024\nshort_model = ShortHFModel(\n    model_name=\"meta-llama/Llama-3.2-3B\",\n    layers_path=\"model.layers\",\n    n_prune_layers=5,\n)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:31.136866Z","iopub.status.busy":"2025-03-15T11:56:31.136553Z","iopub.status.idle":"2025-03-15T11:56:58.456145Z","shell.execute_reply":"2025-03-15T11:56:58.455209Z","shell.execute_reply.started":"2025-03-15T11:56:31.136842Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9377a1e718774594b3dfec4b9bc70904","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10ca2995e4064cffa5b51f5d2e633ed0","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da06c6dd7dd1407eb9c9ef005334f634","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e555d6461e614aab9f5ed0b39f10c7f0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12b0879f9fb84b4e9e7b9639dd8c2595","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cf7b15c480d48bf9bbc272f3793f122","version_major":2,"version_minor":0},"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35a076bd8ddd409cbe435228844159fc","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fabf9d1b1e344cedbeb86e49dd8ed8f3","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93576cda5e6d4bb8b52dd98c6bcd001d","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ff4371cab044b2a81f7368405878ab5","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":8},{"cell_type":"code","source":"short_model.model\n","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:58.457389Z","iopub.status.busy":"2025-03-15T11:56:58.457132Z","iopub.status.idle":"2025-03-15T11:56:58.464033Z","shell.execute_reply":"2025-03-15T11:56:58.463255Z","shell.execute_reply.started":"2025-03-15T11:56:58.457368Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(128256, 3072)\n","    (layers): ModuleList(\n","      (0-27): 28 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n","          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n","          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n","          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"execution_count":9},{"cell_type":"code","source":"short_model.model.config","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:58.465676Z","iopub.status.busy":"2025-03-15T11:56:58.465452Z","iopub.status.idle":"2025-03-15T11:57:06.367816Z","shell.execute_reply":"2025-03-15T11:57:06.366667Z","shell.execute_reply.started":"2025-03-15T11:56:58.465656Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 24,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 32.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.51.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"execution_count":10},{"cell_type":"code","source":"# sample generation\ngen = short_model.model.generate(\n    short_model.tokenizer([\"Dhaka is the capital city of\"], return_tensors='pt').input_ids.to(\"cuda\"),\n    max_new_tokens=50\n)\nshort_model.tokenizer.batch_decode(gen, skip_special_tokens=True)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:57:06.369551Z","iopub.status.busy":"2025-03-15T11:57:06.369221Z","iopub.status.idle":"2025-03-15T11:57:09.155863Z","shell.execute_reply":"2025-03-15T11:57:09.155128Z","shell.execute_reply.started":"2025-03-15T11:57:06.369517Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"data":{"text/plain":["['Dhaka is the capital city of Bangladesh. It is the largest city of Bangladesh and is also the financial and cultural hub of the country. It is a metropolitan city with a population of more than 10 million. The city is divided into two parts, the old city and the new']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"execution_count":11},{"cell_type":"markdown","source":"### üîç Evaluating Token Importance\n\nNow, let's evaluate the **importance of tokens** in our dataset!\n\nHere‚Äôs what we are doing in this step:\n- We loop through each batch in our `dataloader`.\n- For each batch, we take the **text prompts**.\n- We use the model's `eval_importance()` function to check how important each token is in the input.\n\nWe pass these parameters:\n- `prompts`: The text data from our dataset.\n- `max_seq_len=MAX_SEQ_LEN`: The maximum sequence length (1024).\n- `stride=256`: This controls how much the window moves across the text. Smaller strides = more overlap.\n- `max_gen_len=0`: We set this to 0 because we are not generating new text here ‚Äî just evaluating.\n\n> üí° This step helps the model understand which parts of the input text are the most useful, so it can focus on important information during training or pruning!\n\nThe `tqdm` progress bar will show us how the process is going. üöÄ\n","metadata":{}},{"cell_type":"code","source":"for i, batch in enumerate(tqdm(dataloader)):\n    prompts = batch['text']\n\n    short_model.eval_importance(\n        prompts=prompts,\n        max_seq_len=MAX_SEQ_LEN,\n        stride=256,\n        max_gen_len=0\n    )","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:57:09.156889Z","iopub.status.busy":"2025-03-15T11:57:09.156659Z","iopub.status.idle":"2025-03-15T13:21:36.486947Z","shell.execute_reply":"2025-03-15T13:21:36.486071Z","shell.execute_reply.started":"2025-03-15T11:57:09.156870Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7175571dfc664e648b23e75041ad7bdb","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (429039 > 131072). Running this sequence through the model will result in indexing errors\n"]}],"execution_count":12},{"cell_type":"code","source":"short_model.importances","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.488115Z","iopub.status.busy":"2025-03-15T13:21:36.487777Z","iopub.status.idle":"2025-03-15T13:21:36.493352Z","shell.execute_reply":"2025-03-15T13:21:36.492673Z","shell.execute_reply.started":"2025-03-15T13:21:36.488082Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[10885348.2734375,\n"," 4647563.234375,\n"," 3736188.578125,\n"," 3694681.962890625,\n"," 3917046.2265625,\n"," 3819230.4384765625,\n"," 3642028.6752929688,\n"," 3543967.5844726562,\n"," 2997535.2583007812,\n"," 2868903.3413085938,\n"," 2771850.0830078125,\n"," 3112493.9892578125,\n"," 2694467.1118164062,\n"," 2916800.7465820312,\n"," 2807367.796875,\n"," 2245364.837890625,\n"," 1786318.9145507812,\n"," 1495307.1870117188,\n"," 1404219.2998046875,\n"," 1446112.8139648438,\n"," 1077190.0073242188,\n"," 925231.9125976562,\n"," 860718.6850585938,\n"," 903481.4384765625,\n"," 988853.53125,\n"," 1140624.490234375,\n"," 1498234.85546875,\n"," 5314456.3046875]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"execution_count":13},{"cell_type":"code","source":"param_size = 0\nfor param in short_model.model.parameters():\n    param_size += param.nelement() * param.element_size()\n\nbuffer_size = 0\nfor buffer in short_model.model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\n# Total size in bytes to GB\nsize_all_gb = (param_size + buffer_size) / 1024**3\nprint('Model size: {:.3f} GB'.format(size_all_gb))","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.494456Z","iopub.status.busy":"2025-03-15T13:21:36.494145Z","iopub.status.idle":"2025-03-15T13:21:36.512252Z","shell.execute_reply":"2025-03-15T13:21:36.511477Z","shell.execute_reply.started":"2025-03-15T13:21:36.494426Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model size: 5.984 GB\n"]}],"execution_count":14},{"cell_type":"code","source":"short_model.remove_layers()","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.514514Z","iopub.status.busy":"2025-03-15T13:21:36.514264Z","iopub.status.idle":"2025-03-15T13:21:36.528446Z","shell.execute_reply":"2025-03-15T13:21:36.527578Z","shell.execute_reply.started":"2025-03-15T13:21:36.514487Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[22, 23, 21, 24, 20]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"execution_count":15},{"cell_type":"code","source":"short_model.layers","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.529979Z","iopub.status.busy":"2025-03-15T13:21:36.529692Z","iopub.status.idle":"2025-03-15T13:21:36.546087Z","shell.execute_reply":"2025-03-15T13:21:36.545485Z","shell.execute_reply.started":"2025-03-15T13:21:36.529949Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ModuleList(\n","  (0-22): 23 x LlamaDecoderLayer(\n","    (self_attn): LlamaAttention(\n","      (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n","      (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n","      (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n","      (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n","    )\n","    (mlp): LlamaMLP(\n","      (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n","      (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n","      (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n","      (act_fn): SiLU()\n","    )\n","    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n","  )\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"execution_count":16},{"cell_type":"code","source":"short_model.model.config","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.547042Z","iopub.status.busy":"2025-03-15T13:21:36.546828Z","iopub.status.idle":"2025-03-15T13:21:36.562462Z","shell.execute_reply":"2025-03-15T13:21:36.561808Z","shell.execute_reply.started":"2025-03-15T13:21:36.547013Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 24,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 32.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.51.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"execution_count":17},{"cell_type":"code","source":"# reassign layer_idx to attentions for caching\nfor layer_idx, module in enumerate(short_model.layers):\n    module.self_attn.layer_idx = layer_idx","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.563669Z","iopub.status.busy":"2025-03-15T13:21:36.563376Z","iopub.status.idle":"2025-03-15T13:21:36.576507Z","shell.execute_reply":"2025-03-15T13:21:36.575830Z","shell.execute_reply.started":"2025-03-15T13:21:36.563642Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"short_model.model.config","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.577610Z","iopub.status.busy":"2025-03-15T13:21:36.577339Z","iopub.status.idle":"2025-03-15T13:21:36.592971Z","shell.execute_reply":"2025-03-15T13:21:36.592238Z","shell.execute_reply.started":"2025-03-15T13:21:36.577591Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 24,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 32.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.51.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"execution_count":19},{"cell_type":"code","source":"gen = short_model.model.generate(\n    short_model.tokenizer([\"Dhaka is the capital city of\"], return_tensors='pt').input_ids.to(\"cuda\"),\n    max_new_tokens=50,\n    use_cache=True\n)\nshort_model.tokenizer.batch_decode(gen, skip_special_tokens=True)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.593847Z","iopub.status.busy":"2025-03-15T13:21:36.593663Z","iopub.status.idle":"2025-03-15T13:21:38.043070Z","shell.execute_reply":"2025-03-15T13:21:38.042375Z","shell.execute_reply.started":"2025-03-15T13:21:36.593831Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"data":{"text/plain":["['Dhaka is the capital city of Bangladesh. It is the largest city and the largest metropolitan area in Bangladesh. The name means‚Äù‚Äù\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"execution_count":20},{"cell_type":"code","source":"param_size = 0\nfor param in short_model.model.parameters():\n    param_size += param.nelement() * param.element_size()\n\nbuffer_size = 0\nfor buffer in short_model.model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\n# Total size in bytes to GB\nsize_all_gb = (param_size + buffer_size) / 1024**3\nprint('Model size: {:.3f} GB'.format(size_all_gb))","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:34:31.277602Z","iopub.status.busy":"2025-03-15T13:34:31.277254Z","iopub.status.idle":"2025-03-15T13:34:31.284344Z","shell.execute_reply":"2025-03-15T13:34:31.283507Z","shell.execute_reply.started":"2025-03-15T13:34:31.277573Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model size: 5.047 GB\n"]}],"execution_count":21},{"cell_type":"code","source":"short_model.model.config.num_hidden_layers = len(short_model.layers)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:43:44.218564Z","iopub.status.busy":"2025-03-15T13:43:44.218245Z","iopub.status.idle":"2025-03-15T13:43:44.222230Z","shell.execute_reply":"2025-03-15T13:43:44.221395Z","shell.execute_reply.started":"2025-03-15T13:43:44.218541Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"short_model.model.config\n","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:43:53.811944Z","iopub.status.busy":"2025-03-15T13:43:53.811645Z","iopub.status.idle":"2025-03-15T13:43:53.817781Z","shell.execute_reply":"2025-03-15T13:43:53.816949Z","shell.execute_reply.started":"2025-03-15T13:43:53.811920Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 24,\n","  \"num_hidden_layers\": 23,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 32.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.51.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"execution_count":23},{"cell_type":"code","source":"param_size = 0\nfor param in short_model.model.parameters():\n    param_size += param.nelement() * param.element_size()\n\nbuffer_size = 0\nfor buffer in short_model.model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\n# Total size in bytes to GB\nsize_all_gb = (param_size + buffer_size) / 1024**3\nprint('Model size: {:.3f} GB'.format(size_all_gb))","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:44:13.597868Z","iopub.status.busy":"2025-03-15T13:44:13.597580Z","iopub.status.idle":"2025-03-15T13:44:13.604681Z","shell.execute_reply":"2025-03-15T13:44:13.603805Z","shell.execute_reply.started":"2025-03-15T13:44:13.597845Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model size: 5.047 GB\n"]}],"execution_count":24},{"cell_type":"markdown","source":"### üíæ Saving the Pruned Model\n\nAfter evaluating token importance and pruning the model, we now save our work!\n\nHere‚Äôs what‚Äôs happening:\n- We define a name for our new, pruned model: **`shortgpt_llama3_5L_50`**.\n- We check if the output directory exists. If not, we create it.\n- We save both:\n  - The **model weights**.\n  - The **tokenizer** (needed to process text inputs for the model).\n\n> üìù Note: There is also an optional line to save the model configuration (`new_config.save_pretrained()`), but it's currently commented out.\n\nFinally, we print a message to confirm that our pruned model has been saved successfully!\n\nNow, we can load and use this smaller, faster model anytime üöÄ\n","metadata":{}},{"cell_type":"code","source":"import os\nnew_model_name = 'shortgpt_llama3_5L_50'\noutput_dir = './'+new_model_name\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nshort_model.model.save_pretrained(output_dir)\nshort_model.tokenizer.save_pretrained(output_dir)\n#new_config.save_pretrained(output_dir)\nprint(f\"Pruned model saved to {output_dir}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:46:40.328778Z","iopub.status.busy":"2025-03-15T13:46:40.328456Z","iopub.status.idle":"2025-03-15T13:46:56.177865Z","shell.execute_reply":"2025-03-15T13:46:56.176561Z","shell.execute_reply.started":"2025-03-15T13:46:40.328755Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Pruned model saved to ./shortgpt_llama3_5L_50\n"]}],"execution_count":25},{"cell_type":"code","source":"# Push the model to your Hugging Face repository\n\nshort_model.model.push_to_hub(new_model_name, private=False)\nshort_model.tokenizer.push_to_hub(new_model_name)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:46:56.179064Z","iopub.status.busy":"2025-03-15T13:46:56.178768Z","iopub.status.idle":"2025-03-15T13:47:55.669023Z","shell.execute_reply":"2025-03-15T13:47:55.668119Z","shell.execute_reply.started":"2025-03-15T13:46:56.179040Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f1da347859f4780badf447ae1cc741f","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"833110a17fa14ca2a54fc774d71aa7c5","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/453M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d57082351a54c6397dcf7824d043f9e","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db89b595592d421092e3d453ed2460fc","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9aad2ad34984ac790f63561a4404cb4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/rahatneuron/shortgpt_llama3_5L_50/commit/51abf2eefd8f04c9feaa060bc58bf29aa6803911', commit_message='Upload tokenizer', commit_description='', oid='51abf2eefd8f04c9feaa060bc58bf29aa6803911', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rahatneuron/shortgpt_llama3_5L_50', endpoint='https://huggingface.co', repo_type='model', repo_id='rahatneuron/shortgpt_llama3_5L_50'), pr_revision=None, pr_num=None)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}