{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ðŸ” LLaMA 3 Pruning for Efficient Inference\n\nThis project explores structured pruning of the LLaMA 3 8B model to reduce its size and improve inference efficiency. Using block-level influence scores based on hidden state similarity, we identify and remove the least important layers while maintaining performance on downstream tasks like HellaSwag.\n\nThe workflow includes:\n- Loading and preparing the model\n- Measuring influence of transformer blocks\n- Pruning selected layers\n- Evaluating the pruned model using `lm_eval`\n\nThis notebook provides a reproducible implementation of the pruning pipeline, along with benchmarks to compare model performance before and after pruning.\n","metadata":{}},{"cell_type":"code","source":"!pip install datasets lm_eval","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-03-15T11:55:33.273404Z","iopub.status.busy":"2025-03-15T11:55:33.273125Z","iopub.status.idle":"2025-03-15T11:55:48.051101Z","shell.execute_reply":"2025-03-15T11:55:48.050309Z","shell.execute_reply.started":"2025-03-15T11:55:33.273381Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.5.0)\n","Requirement already satisfied: lm_eval in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.8)\n","Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (19.0.1)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.2)\n","Requirement already satisfied: accelerate>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (1.6.0)\n","Requirement already satisfied: evaluate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (0.4.3)\n","Requirement already satisfied: jsonlines in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (4.0.0)\n","Requirement already satisfied: numexpr in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (2.10.2)\n","Requirement already satisfied: peft>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (0.15.1)\n","Requirement already satisfied: pybind11>=2.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (2.13.6)\n","Requirement already satisfied: pytablewriter in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (1.2.1)\n","Requirement already satisfied: rouge-score>=0.0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (0.1.2)\n","Requirement already satisfied: sacrebleu>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (2.5.1)\n","Requirement already satisfied: scikit-learn>=0.24.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (1.3.2)\n","Requirement already satisfied: sqlitedict in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (2.1.0)\n","Requirement already satisfied: torch>=1.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (2.2.1+cu121)\n","Requirement already satisfied: tqdm-multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (0.0.11)\n","Requirement already satisfied: transformers>=4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (4.51.2)\n","Requirement already satisfied: zstandard in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (0.23.0)\n","Requirement already satisfied: word2number in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (1.1)\n","Requirement already satisfied: more_itertools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lm_eval) (10.6.0)\n","Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0->lm_eval) (7.0.0)\n","Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=0.26.0->lm_eval) (0.5.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval) (2.1.0)\n","Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval) (3.9.1)\n","Requirement already satisfied: six>=1.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval) (1.17.0)\n","Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval) (3.1.1)\n","Requirement already satisfied: regex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval) (2024.11.6)\n","Requirement already satisfied: tabulate>=0.8.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval) (0.9.0)\n","Requirement already satisfied: colorama in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval) (0.4.6)\n","Requirement already satisfied: lxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval) (5.3.2)\n","Requirement already satisfied: scipy>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn>=0.24.1->lm_eval) (3.6.0)\n","Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (1.13.3)\n","Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (3.4.2)\n","Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.8->lm_eval) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm_eval) (12.8.93)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.1->lm_eval) (0.21.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: setuptools>=38.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval) (75.8.0)\n","Requirement already satisfied: DataProperty<2,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval) (1.1.0)\n","Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval) (1.1.4)\n","Requirement already satisfied: pathvalidate<4,>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval) (3.2.3)\n","Requirement already satisfied: tabledata<2,>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval) (1.3.4)\n","Requirement already satisfied: tcolorpy<1,>=0.0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytablewriter->lm_eval) (0.1.7)\n","Requirement already satisfied: typepy<2,>=1.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval) (1.3.4)\n","Requirement already satisfied: chardet<6,>=3.0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval) (5.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.8->lm_eval) (3.0.2)\n","Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score>=0.0.4->lm_eval) (8.1.8)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.8->lm_eval) (1.3.0)\n"]}],"execution_count":1},{"cell_type":"code","source":"# importing the required libraries\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, Trainer, TrainingArguments\nfrom collections import OrderedDict\nfrom typing import List, Optional\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nimport torch\nfrom torch.utils.data import DataLoader\nfrom lm_eval import evaluator, tasks","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:55:48.052355Z","iopub.status.busy":"2025-03-15T11:55:48.052047Z","iopub.status.idle":"2025-03-15T11:56:11.087829Z","shell.execute_reply":"2025-03-15T11:56:11.087111Z","shell.execute_reply.started":"2025-03-15T11:55:48.052332Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:11.088701Z","iopub.status.busy":"2025-03-15T11:56:11.088489Z","iopub.status.idle":"2025-03-15T11:56:11.115810Z","shell.execute_reply":"2025-03-15T11:56:11.114927Z","shell.execute_reply.started":"2025-03-15T11:56:11.088683Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a467ed81ca94889b30827b10180d4b5","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token has not been saved to git credential helper.\n"]}],"execution_count":3},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def layer_removal(\n    model: nn.Module,\n    layers_to_remove: OrderedDict\n):\n    \"\"\"\n    Generic removal implementation\n    \"\"\"\n\n    for layer_name, layer_idx in layers_to_remove.items():\n        modules = layer_name.split(\".\")\n        mod = model\n        for m in modules[:-1]:\n            mod = getattr(mod, m)\n        \n        if layer_idx is None:\n            delattr(mod, modules[-1])\n        else:\n            delattr(mod, modules[-1])[layer_idx]","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:17.163635Z","iopub.status.busy":"2025-03-15T11:56:17.163311Z","iopub.status.idle":"2025-03-15T11:56:17.168457Z","shell.execute_reply":"2025-03-15T11:56:17.167616Z","shell.execute_reply.started":"2025-03-15T11:56:17.163610Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### `block_influence`\n\nMeasures how much a layer changes the hidden states.\n\n- Takes input and output hidden states from a layer.\n- Calculates similarity between them.\n- If `angular=True`, uses angle-based distance.\n- Otherwise, returns 1 - cosine similarity.\n\nUsed to find less important layers for pruning.\n","metadata":{}},{"cell_type":"code","source":"def block_influence(\n    input_hidden_state: torch.Tensor,\n    output_hidden_state: torch.Tensor,\n    angular=False,\n):\n    \"\"\"\n    input_hidden_state: B, S, D\n    output_hidden_state: B, S, D\n    \"\"\"\n    _, _, d = input_hidden_state.shape\n    input_hidden_state = input_hidden_state.reshape(-1, d)\n    output_hidden_state = output_hidden_state.reshape(-1, d)\n\n    norm_input = input_hidden_state.norm(dim=-1, keepdim=True)\n    norm_output = output_hidden_state.norm(dim=-1, keepdim=True)\n\n    sim = (input_hidden_state @ output_hidden_state.T) / (norm_input * norm_output)\n    sim = sim.diagonal().nan_to_num(nan=0.5)\n\n    if angular:\n        return (torch.arccos(sim) / torch.pi)\n\n    return 1 - sim","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:18.125812Z","iopub.status.busy":"2025-03-15T11:56:18.125501Z","iopub.status.idle":"2025-03-15T11:56:18.130773Z","shell.execute_reply":"2025-03-15T11:56:18.129891Z","shell.execute_reply.started":"2025-03-15T11:56:18.125787Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### `ShortHFModel` Class\n\nA wrapper around HuggingFace's language models for pruning transformer layers based on their importance.\n\n#### `__init__`\n- Loads a HuggingFace model and tokenizer.\n- Finds the model layers using the provided path (e.g. `\"model.layers\"`).\n- Sets up a list to store layer importance scores.\n\n#### `remove_layers()`\n- Removes a number of layers based on importance.\n- If `angular=True`, picks consecutive layers with least angular impact.\n- Otherwise, removes the least important layers globally.\n- Layers are removed in reverse to avoid index errors.\n\n#### `compute_bi()`\n- Calculates how much each layer changes the hidden states.\n- Uses the `block_influence()` function.\n\n#### `eval_importance()`\n- Feeds input prompts into the model using a sliding window.\n- Collects hidden states across layers without generating output.\n- Computes and updates layer importance scores from the outputs.\n\nUsed to rank and prune LLaMA layers in a structured way.\n","metadata":{}},{"cell_type":"code","source":"class ShortHFModel():\n\n    def __init__(self, model_name: str, layers_path: str, n_prune_layers: Optional[int] = None):\n        \"\"\"\n        HuggingFace Model Wrapper\n\n        Args:\n            model_name (str): HuggingFace model name\n            layers_path (str): String in dot notation demonstrating how to access layers of the model. Ex: \"model.layers\"\n            (Optional) n_prune_layers (int): Number of layers to prune. Defaults to None.\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n        # self.model.params = self.model.to_fp16(self.model.params)\n        self.model.to(\"cuda\")\n\n        modules = layers_path.split(\".\")\n        mod = self.model\n        for m in modules:\n            mod = getattr(mod, m)\n        self.layers = mod\n\n        self.n_prune_layers = n_prune_layers\n        self.importances = [0 for _ in self.layers]  # layer-wise importance scores\n\n    def remove_layers(\n        self,\n        layers_to_remove: Optional[List[int]] = [],\n        angular: Optional[bool] = False\n    ):\n        if angular:\n            assert self.importances, \"Need to compute importances with eval_importance()\"\n            assert self.n_prune_layers, \"Need number of layers to prune, set `n_prune_layers`\"\n            start_layer = np.argsort(np.array(self.importances[:-self.n_prune_layers+1]))[0]\n            layers_to_remove = list(range(start_layer, start_layer + self.n_prune_layers))\n        elif not layers_to_remove and self.n_prune_layers:\n            assert self.importances, \"Need to compute importances with eval_importance()\"\n            layers_to_remove = np.argsort(np.array(self.importances))[:self.n_prune_layers].tolist()\n\n        # remove layers in reverse to avoid indexing errors\n        for layer_idx in sorted(layers_to_remove, reverse=True):\n            try:\n                del self.layers[layer_idx]\n            except IndexError:\n                print(f\"layer {layer_idx} does not exist, function may have already been called\")\n                return []\n        \n        return layers_to_remove\n    \n    def compute_bi(self, hiddens: List[torch.Tensor], angular: bool):\n        n = 1\n        if angular:\n            assert self.n_prune_layers is not None, \"Set number of layers to prune to use angular importance\"\n            n = self.n_prune_layers\n\n        for i in range(len(hiddens) - n):\n            in_hidden = hiddens[i]\n            out_hidden = hiddens[i+n]\n            if angular:\n                # use only last token for angular distance as described in section 3.2\n                # https://arxiv.org/pdf/2403.17887.pdf\n                in_hidden = in_hidden[:,-1:]\n                out_hidden = out_hidden[:,-1:]\n            \n            self.importances[i] += block_influence(\n                in_hidden,\n                out_hidden,\n                angular=angular\n            ).sum().cpu().item()\n\n    @torch.inference_mode()\n    def eval_importance(\n        self,\n        prompts: List[str],\n        max_seq_len: int,\n        stride: int = 256,\n        max_gen_len: int = 0,\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        angular: Optional[bool] = False\n    ):\n        \"\"\"\n        Computes layer-wise importances over input texts.\n\n        NOTE: ShortGPT paper performs no generation during importance computation, which suggests a `max_gen_len`= 0.\n\n        Args:\n            prompts (List[str]): List of prompts.\n            max_seq_len (int): Maximum sequence length for model input, the sliding window size.\n            (Optional) stride (int): Number of tokens to skip/shift between each window inference.\n            (Optional) max_gen_len (int): Maximum length of the generated text sequence.\n            (Optional) temperature (float): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            (Optional) top_p (float): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            (Optional) angular (bool): Whether to ues angular distance. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        prompt_tokens = self.tokenizer(\n            prompts,\n            padding=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        input_ids = prompt_tokens.input_ids\n        attn_mask = prompt_tokens.attention_mask\n\n        max_prompt_len = max(len(t) for t in input_ids)\n\n        # authors use a sliding window of size 1024 with a shift of 256\n        for start in range(0, max_prompt_len, stride):\n            seq_ids = (attn_mask.sum(dim=-1) > start).nonzero().squeeze()\n            seq_ids = seq_ids.unsqueeze(0) if seq_ids.dim() == 0 else seq_ids  # ensure 2d\n            inputs = input_ids[seq_ids, start:start+max_seq_len]\n            attn = attn_mask[seq_ids, start:start+max_seq_len]\n\n            if max_gen_len == 0:\n                outputs = self.model(\n                    input_ids=inputs.to(\"cuda\"),\n                    attention_mask=attn.to(\"cuda\"),\n                    output_hidden_states=True,\n                )\n            else:\n                outputs = self.model.generate(\n                    input_ids=inputs.to(\"cuda\"),\n                    attention_mask=attn.to(\"cuda\"),\n                    max_new_tokens=max_gen_len, \n                    do_sample=True,\n                    temperature=temperature,\n                    top_p=top_p,\n                    output_hidden_states=True,\n                    return_dict_in_generate=True,\n                )\n            \n            self.compute_bi(outputs.hidden_states, angular=angular)\n\n        return","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:20.250357Z","iopub.status.busy":"2025-03-15T11:56:20.249767Z","iopub.status.idle":"2025-03-15T11:56:20.275092Z","shell.execute_reply":"2025-03-15T11:56:20.274217Z","shell.execute_reply.started":"2025-03-15T11:56:20.250309Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Step 1: Load dataset\ndata = load_dataset(\"hellaswag\", split=\"validation\", trust_remote_code=True)\n\ndef process_hellaswag(examples):\n    texts = []\n    for ctx, endings_list, label in zip(examples[\"ctx\"], examples[\"endings\"], examples[\"label\"]):\n        combined_text = ctx + \" \" + endings_list[int(label)]  # âœ… Convert label to int\n        texts.append(combined_text)\n    return {\"text\": texts}\n\n\n\nprocessed_data = data.map(process_hellaswag, batched=True)\n\n# âœ… Step 3: Create DataLoader **after processing**\ndataloader = DataLoader(\n    processed_data,\n    batch_size=1,\n)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:23.778673Z","iopub.status.busy":"2025-03-15T11:56:23.778340Z","iopub.status.idle":"2025-03-15T11:56:29.863135Z","shell.execute_reply":"2025-03-15T11:56:29.862458Z","shell.execute_reply.started":"2025-03-15T11:56:23.778644Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Model Setup\n\nInitializes the pruning wrapper with the base LLaMA 3.1 8B model.\n\n- `MAX_SEQ_LEN = 1024`: Defines the window size for input sequences.\n- `ShortHFModel(...)`: Loads the model and prepares to prune 5 layers from `model.layers`.\n","metadata":{}},{"cell_type":"code","source":"MAX_SEQ_LEN = 1024\nshort_model = ShortHFModel(\n    model_name=\"meta-llama/Llama-3.1-8B\",\n    layers_path=\"model.layers\",\n    n_prune_layers=5,\n)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:31.136866Z","iopub.status.busy":"2025-03-15T11:56:31.136553Z","iopub.status.idle":"2025-03-15T11:56:58.456145Z","shell.execute_reply":"2025-03-15T11:56:58.455209Z","shell.execute_reply.started":"2025-03-15T11:56:31.136842Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"202c5fd36c3b4995b8d1b487ee190042","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ed53f10b5014e179ae561871e257f18","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a50052ec4597456d842ddf43bed56d8d","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"116c08fbb89c4001b02655b2b16d422b","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"929b7309a76f4987baa05251ecbd4392","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee619893cfe840b9bea189f44f669b25","version_major":2,"version_minor":0},"text/plain":["Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67a708c5879541beb2370018fc55e4f6","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0de7e1aa0768491f99fee0e3343cfa34","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9b1aaa107f9494282e86ac6fa51a2d5","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b244c10004fd472d96b404b8324db980","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2ab4dbfc7874728a30d5eaf929e7ab1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fa067a598b1403eb7c3ed877f77be84","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":8},{"cell_type":"code","source":"short_model.model\n","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:58.457389Z","iopub.status.busy":"2025-03-15T11:56:58.457132Z","iopub.status.idle":"2025-03-15T11:56:58.464033Z","shell.execute_reply":"2025-03-15T11:56:58.463255Z","shell.execute_reply.started":"2025-03-15T11:56:58.457368Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(128256, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"execution_count":9},{"cell_type":"code","source":"short_model.model.config","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:56:58.465676Z","iopub.status.busy":"2025-03-15T11:56:58.465452Z","iopub.status.idle":"2025-03-15T11:57:06.367816Z","shell.execute_reply":"2025-03-15T11:57:06.366667Z","shell.execute_reply.started":"2025-03-15T11:56:58.465656Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.51.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"execution_count":10},{"cell_type":"code","source":"# sample generation\ngen = short_model.model.generate(\n    short_model.tokenizer([\"Dhaka is the capital city of\"], return_tensors='pt').input_ids.to(\"cuda\"),\n    max_new_tokens=50\n)\nshort_model.tokenizer.batch_decode(gen, skip_special_tokens=True)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:57:06.369551Z","iopub.status.busy":"2025-03-15T11:57:06.369221Z","iopub.status.idle":"2025-03-15T11:57:09.155863Z","shell.execute_reply":"2025-03-15T11:57:09.155128Z","shell.execute_reply.started":"2025-03-15T11:57:06.369517Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"data":{"text/plain":["['Dhaka is the capital city of Bangladesh, located in the center of the country. It is the largest city in Bangladesh and one of the most densely populated cities in the world. Dhaka is a bustling metropolis with a rich cultural heritage and a thriving economy.\\nDhaka is']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"execution_count":11},{"cell_type":"code","source":"# Step 4: Run importance evaluation\nfor i, batch in enumerate(tqdm(dataloader)):\n    prompts = batch['text']\n\n    short_model.eval_importance(\n        prompts=prompts,\n        max_seq_len=256,\n        stride=256,\n        max_gen_len=0,\n        angular=True  # âœ… angular enabled\n    )","metadata":{"execution":{"iopub.execute_input":"2025-03-15T11:57:09.156889Z","iopub.status.busy":"2025-03-15T11:57:09.156659Z","iopub.status.idle":"2025-03-15T13:21:36.486947Z","shell.execute_reply":"2025-03-15T13:21:36.486071Z","shell.execute_reply.started":"2025-03-15T11:57:09.156870Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10042/10042 [05:54<00:00, 28.31it/s]\n"]}],"execution_count":12},{"cell_type":"code","source":"short_model.importances","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.488115Z","iopub.status.busy":"2025-03-15T13:21:36.487777Z","iopub.status.idle":"2025-03-15T13:21:36.493352Z","shell.execute_reply":"2025-03-15T13:21:36.492673Z","shell.execute_reply.started":"2025-03-15T13:21:36.488082Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[4596.36279296875,\n"," 3559.147216796875,\n"," 3543.085205078125,\n"," 3428.084228515625,\n"," 3486.94287109375,\n"," 3372.723876953125,\n"," 3314.354736328125,\n"," 3309.061767578125,\n"," 3233.232666015625,\n"," 3259.996826171875,\n"," 3117.794921875,\n"," 3121.95751953125,\n"," 3185.0390625,\n"," 3159.41552734375,\n"," 2998.45654296875,\n"," 2843.5714111328125,\n"," 2648.7491455078125,\n"," 2451.3165283203125,\n"," 2269.86328125,\n"," 2119.666748046875,\n"," 1983.0262451171875,\n"," 1926.4088134765625,\n"," 1806.6669921875,\n"," 1745.3758544921875,\n"," 1764.9443359375,\n"," 1890.9305419921875,\n"," 2553.4669189453125,\n"," 4105.641357421875,\n"," 0,\n"," 0,\n"," 0,\n"," 0]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"execution_count":13},{"cell_type":"code","source":"param_size = 0\nfor param in short_model.model.parameters():\n    param_size += param.nelement() * param.element_size()\n\nbuffer_size = 0\nfor buffer in short_model.model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\n# Total size in bytes to GB\nsize_all_gb = (param_size + buffer_size) / 1024**3\nprint('Model size: {:.3f} GB'.format(size_all_gb))","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.494456Z","iopub.status.busy":"2025-03-15T13:21:36.494145Z","iopub.status.idle":"2025-03-15T13:21:36.512252Z","shell.execute_reply":"2025-03-15T13:21:36.511477Z","shell.execute_reply.started":"2025-03-15T13:21:36.494426Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model size: 14.958 GB\n"]}],"execution_count":14},{"cell_type":"code","source":"short_model.remove_layers(angular=True)\n","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.514514Z","iopub.status.busy":"2025-03-15T13:21:36.514264Z","iopub.status.idle":"2025-03-15T13:21:36.528446Z","shell.execute_reply":"2025-03-15T13:21:36.527578Z","shell.execute_reply.started":"2025-03-15T13:21:36.514487Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[23, 24, 25, 26, 27]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"execution_count":15},{"cell_type":"code","source":"short_model.layers","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.529979Z","iopub.status.busy":"2025-03-15T13:21:36.529692Z","iopub.status.idle":"2025-03-15T13:21:36.546087Z","shell.execute_reply":"2025-03-15T13:21:36.545485Z","shell.execute_reply.started":"2025-03-15T13:21:36.529949Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ModuleList(\n","  (0-26): 27 x LlamaDecoderLayer(\n","    (self_attn): LlamaAttention(\n","      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n","      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n","    )\n","    (mlp): LlamaMLP(\n","      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n","      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n","      (act_fn): SiLU()\n","    )\n","    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n","    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n","  )\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"execution_count":16},{"cell_type":"code","source":"short_model.model.config","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.547042Z","iopub.status.busy":"2025-03-15T13:21:36.546828Z","iopub.status.idle":"2025-03-15T13:21:36.562462Z","shell.execute_reply":"2025-03-15T13:21:36.561808Z","shell.execute_reply.started":"2025-03-15T13:21:36.547013Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.51.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"execution_count":17},{"cell_type":"code","source":"# reassign layer_idx to attentions for caching\nfor layer_idx, module in enumerate(short_model.layers):\n    module.self_attn.layer_idx = layer_idx","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.563669Z","iopub.status.busy":"2025-03-15T13:21:36.563376Z","iopub.status.idle":"2025-03-15T13:21:36.576507Z","shell.execute_reply":"2025-03-15T13:21:36.575830Z","shell.execute_reply.started":"2025-03-15T13:21:36.563642Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"short_model.model.config","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.577610Z","iopub.status.busy":"2025-03-15T13:21:36.577339Z","iopub.status.idle":"2025-03-15T13:21:36.592971Z","shell.execute_reply":"2025-03-15T13:21:36.592238Z","shell.execute_reply.started":"2025-03-15T13:21:36.577591Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.51.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"execution_count":19},{"cell_type":"code","source":"gen = short_model.model.generate(\n    short_model.tokenizer([\"Dhaka is the capital city of\"], return_tensors='pt').input_ids.to(\"cuda\"),\n    max_new_tokens=50,\n    use_cache=True\n)\nshort_model.tokenizer.batch_decode(gen, skip_special_tokens=True)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:21:36.593847Z","iopub.status.busy":"2025-03-15T13:21:36.593663Z","iopub.status.idle":"2025-03-15T13:21:38.043070Z","shell.execute_reply":"2025-03-15T13:21:38.042375Z","shell.execute_reply.started":"2025-03-15T13:21:36.593831Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"data":{"text/plain":["['Dhaka is the capital city of Bangladesh and the largest city in the South East Asia. It is located in the central central-eastern region of the country and is the largest urban area in the eastern hemisphere.\\nThe city is the largest city in the South East Asia. It is located']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"execution_count":20},{"cell_type":"code","source":"param_size = 0\nfor param in short_model.model.parameters():\n    param_size += param.nelement() * param.element_size()\n\nbuffer_size = 0\nfor buffer in short_model.model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\n# Total size in bytes to GB\nsize_all_gb = (param_size + buffer_size) / 1024**3\nprint('Model size: {:.3f} GB'.format(size_all_gb))","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:34:31.277602Z","iopub.status.busy":"2025-03-15T13:34:31.277254Z","iopub.status.idle":"2025-03-15T13:34:31.284344Z","shell.execute_reply":"2025-03-15T13:34:31.283507Z","shell.execute_reply.started":"2025-03-15T13:34:31.277573Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model size: 12.926 GB\n"]}],"execution_count":21},{"cell_type":"code","source":"short_model.model.config.num_hidden_layers = len(short_model.layers)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:43:44.218564Z","iopub.status.busy":"2025-03-15T13:43:44.218245Z","iopub.status.idle":"2025-03-15T13:43:44.222230Z","shell.execute_reply":"2025-03-15T13:43:44.221395Z","shell.execute_reply.started":"2025-03-15T13:43:44.218541Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"short_model.model.config\n","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:43:53.811944Z","iopub.status.busy":"2025-03-15T13:43:53.811645Z","iopub.status.idle":"2025-03-15T13:43:53.817781Z","shell.execute_reply":"2025-03-15T13:43:53.816949Z","shell.execute_reply.started":"2025-03-15T13:43:53.811920Z"},"trusted":true},"outputs":[{"data":{"text/plain":["LlamaConfig {\n","  \"_attn_implementation_autoset\": true,\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 27,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 8.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float16\",\n","  \"transformers_version\": \"4.51.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"execution_count":23},{"cell_type":"code","source":"param_size = 0\nfor param in short_model.model.parameters():\n    param_size += param.nelement() * param.element_size()\n\nbuffer_size = 0\nfor buffer in short_model.model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\n# Total size in bytes to GB\nsize_all_gb = (param_size + buffer_size) / 1024**3\nprint('Model size: {:.3f} GB'.format(size_all_gb))","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:44:13.597868Z","iopub.status.busy":"2025-03-15T13:44:13.597580Z","iopub.status.idle":"2025-03-15T13:44:13.604681Z","shell.execute_reply":"2025-03-15T13:44:13.603805Z","shell.execute_reply.started":"2025-03-15T13:44:13.597845Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model size: 12.926 GB\n"]}],"execution_count":24},{"cell_type":"code","source":"import os\nnew_model_name = 'shortgpt_llama3.1_8B_hellaswag_angular'\noutput_dir = './'+new_model_name\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nshort_model.model.save_pretrained(output_dir)\nshort_model.tokenizer.save_pretrained(output_dir)\n#new_config.save_pretrained(output_dir)\nprint(f\"Pruned model saved to {output_dir}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:46:40.328778Z","iopub.status.busy":"2025-03-15T13:46:40.328456Z","iopub.status.idle":"2025-03-15T13:46:56.177865Z","shell.execute_reply":"2025-03-15T13:46:56.176561Z","shell.execute_reply.started":"2025-03-15T13:46:40.328755Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Pruned model saved to ./shortgpt_llama3.1_8B_hellaswag_angular\n"]}],"execution_count":25},{"cell_type":"code","source":"# Push the model to your Hugging Face repository\n\nshort_model.model.push_to_hub(new_model_name, private=False)\nshort_model.tokenizer.push_to_hub(new_model_name)","metadata":{"execution":{"iopub.execute_input":"2025-03-15T13:46:56.179064Z","iopub.status.busy":"2025-03-15T13:46:56.178768Z","iopub.status.idle":"2025-03-15T13:47:55.669023Z","shell.execute_reply":"2025-03-15T13:47:55.668119Z","shell.execute_reply.started":"2025-03-15T13:46:56.179040Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab110ee548b64675890cdd0a84d14306","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d6b1aa272984ace8d5db8c844a9c8cc","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6b51a1d9bec49138ba0005d1e617ba2","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a2c8426490a414b91dd064b6880c158","version_major":2,"version_minor":0},"text/plain":["Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3fb1a4494a644bab586660c22932777","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"449f070fce7b47b9a2824491d0280e4a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Shahrukh0/shortgpt_llama3.1_8B_hellaswag_angular/commit/51f44ec9460ca19896cc9a2b5b8b9f8e3ea43a15', commit_message='Upload tokenizer', commit_description='', oid='51f44ec9460ca19896cc9a2b5b8b9f8e3ea43a15', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Shahrukh0/shortgpt_llama3.1_8B_hellaswag_angular', endpoint='https://huggingface.co', repo_type='model', repo_id='Shahrukh0/shortgpt_llama3.1_8B_hellaswag_angular'), pr_revision=None, pr_num=None)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}